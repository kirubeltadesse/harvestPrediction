{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peliminary Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import requests,json, pickle, glob\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that receive the range of dates\n",
    "# and get the data from the folders\n",
    "# and append to the dataframe\n",
    "\n",
    "# oragon = pd.read_excel('https://query.data.world/s/tnmiycnhx4mu4777vps4nphqm4qeoh')\n",
    "# oragon\n",
    "\n",
    "# PATH = r'C:/Users/ktadesse/Google Drive/research/VACCINE/Research Direction/Napa Valley/2016/'\n",
    "\n",
    "# df_rain = pd.read_csv(PATH + 'rainfall.csv', names=['dates', 'inches']) #, index_col='dates')\n",
    "# df_stages = pd.read_csv(PATH+'stages.csv', names=['dates', 'types'])\n",
    "\n",
    "# # resetting the date as an index\n",
    "# rain = df_rain.set_index('dates')\n",
    "# stages = df_stages.set_index('dates')\n",
    "\n",
    "# # convering the date to a date datatype\n",
    "# rain.index = pd.to_datetime(rain.index)\n",
    "# stages.index = pd.to_datetime(stages.index)\n",
    "\n",
    "path = r'C:/Users/ktadesse/Google Drive/research/VACCINE/Research Direction/Napa Valley/'\n",
    "filenames = glob.glob(path +\"*/rainfall.csv\")\n",
    "\n",
    "# big_fame = pd.concat([pd.read_csv(f, names=['dates', 'inches']) for f in glob.glob(path +\"*/rainfall.csv\")],\n",
    "#                     ignore_index=True, sort=Ture)\n",
    "df_rain = pd.concat([pd.read_csv(f, names=['dates','inches']) for f in glob.glob(path +\"*/rainfall.csv\")])\n",
    "df_stages = pd.concat([pd.read_csv(f, names=['dates','types']) for f in glob.glob(path +\"*/stages.csv\")])\n",
    "\n",
    "# resetting the date as an index\n",
    "rain = df_rain.set_index('dates')\n",
    "stages = df_stages.set_index('dates')\n",
    "\n",
    "# convering the date to a date datatype\n",
    "rain.index = pd.to_datetime(rain.index)\n",
    "stages.index = pd.to_datetime(stages.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Fetching data from CMIS API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "API_KEY = 'fc26f8f4-3474-4ffa-b60e-2b39b66a46a0' # '61a06bdc-a5b5-4229-9f61-b91bdff8ca63'  \n",
    "dataItems_list = ['day-air-tmp-avg','day-air-tmp-max','day-air-tmp-min','day-dew-pnt','day-eto','day-asce-eto','day-asce-etr','day-precip','day-sol-rad-avg','day-wind-spd-avg']\n",
    "dataItems = ','.join(dataItems_list)\n",
    "BASE_URL = 'http://et.water.ca.gov/api/data?appKey={}&targets={}&startDate={}&endDate={}&dataItems={}&prioritizeSCS=N'\n",
    "\n",
    "def extract_weather_data(zipcode, start_date, end_date):\n",
    "    records = []\n",
    "    request= BASE_URL.format(API_KEY,zipcode,start_date, end_date, dataItems)\n",
    "    response = requests.get(request)\n",
    "    data =response.json()\n",
    "    if response.status_code == 200:\n",
    "#         data = response.json()\n",
    "        dates, hours, frames, Eto_values, col_names, DayPrecip, DayAirTmpMin, DayAirTmpMax, DayEto, DayAirTmpAvg, DaySolRadAvg, DayWindSpdAvg = ([] for i in range(12))\n",
    "        for i, day in enumerate(data['Data']['Providers'][0]['Records']):\n",
    "            dates.append(day['Date'])\n",
    "            Eto_values.append(day['DayAsceEto']['Value'])\n",
    "            DayPrecip.append(day['DayPrecip']['Value'])\n",
    "            DayAirTmpMin.append(day['DayAirTmpMin']['Value'])\n",
    "            DayAirTmpMax.append(day['DayAirTmpMax']['Value'])\n",
    "            DayEto.append(day['DayEto']['Value'])\n",
    "            DayAirTmpAvg.append(day['DayAirTmpAvg']['Value'])\n",
    "            DaySolRadAvg.append(day['DaySolRadAvg']['Value'])\n",
    "            DayWindSpdAvg.append(day['DayWindSpdAvg']['Value'])\n",
    "            # hours.append(int(day.get('Hour', '0000'))/100)\n",
    "            reqdata = {'DayPrecip': DayPrecip, 'DayAirTmpMin': DayAirTmpMin, 'DayAirTmpMax': DayAirTmpMax,\n",
    "                       'DayEto': DayEto, 'DayAirTmpAvg': DayAirTmpAvg, 'DayAirTmpAvg': DayAirTmpAvg,\n",
    "                       'DaySolRadAvg': DaySolRadAvg, 'DayWindSpdAvg': DayWindSpdAvg, 'Eto_values': Eto_values}\n",
    "            df = pd.DataFrame(reqdata, dtype=float)\n",
    "            df.index = dates\n",
    "        df.index = pd.to_datetime(df.index) \n",
    "        df = df.dropna()\n",
    "    else:\n",
    "        return response   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def weather_mulit_years(start, end):\n",
    "    years = pd.date_range(start, end, freq='Y')\n",
    "    start = start\n",
    "    df_yrs = []\n",
    "    for indx, i in enumerate(years.date):\n",
    "        end = years.date[indx]\n",
    "        df = extract_weather_data('94581',start,end)\n",
    "        df_yrs.append(df)\n",
    "        start = end\n",
    "    dataset = pd.concat([*df_yrs], axis=0)\n",
    "    dataset = dataset.drop_duplicates()\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df = weather_mulit_years('2010-01-01','2017-12-31')\n",
    "\n",
    "# df[df.index.duplicated(keep=False)]\n",
    "\n",
    "# df.iloc['2011-01-09']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to take one type and range of dates\n",
    "# make a dataframe\n",
    "# make index date range\n",
    "# return the dataset of a given datatype\n",
    "def make_dataset(kind, start, end):\n",
    "    df = weather_mulit_years(start, end)\n",
    "    \n",
    "    duration = pd.date_range(start, end, freq='D')\n",
    "    \n",
    "    dataset = pd.DataFrame(columns=['minTemp', 'maxTemp', 'Eto_values','DayPrecip','WindSpd','DaySolRad','rain','type','stage'])\n",
    "\n",
    "    start = np.nan\n",
    "    \n",
    "    #going through the 4 types of wine (Sauvignon Blanc, Chardonnay, Merlot, and Cabernet Sauvignon)\n",
    "#     for kind in set(df_stages['types']):\n",
    "    DATA = {'Start': stages[stages['types'] == kind].index.tolist(),\n",
    "           'Stages':[(i%4) for i in range(stages[stages['types']==1].shape[0])] #['Bud', 'Bloom','Veraison', 'Harvest']\n",
    "           }\n",
    "    dfs = pd.DataFrame(DATA).set_index('Start')\n",
    "\n",
    "#         going through each date\n",
    "    for i in duration.date:\n",
    "        if i in df.index:\n",
    "            entry = pd.DataFrame({'minTemp': df.loc[i,'DayAirTmpMin'],\n",
    "                                 'maxTemp': df.loc[i,'DayAirTmpMax'],\n",
    "                                  'Eto_values':df.loc[i,'Eto_values'],\n",
    "                                  'DayPrecip': df.loc[i,'DayPrecip'],\n",
    "                                  'WindSpd': df.loc[i,'DayWindSpdAvg'],\n",
    "                                  'DaySolRad': df.loc[i,'DaySolRadAvg']\n",
    "                                 }, index=[i])\n",
    "\n",
    "        # adding rain \n",
    "        if(rain.loc[rain.index.month == i.month]['inches'].empty == True):\n",
    "            entry['rain'] = 0.00\n",
    "        else:\n",
    "            entry['rain'] = rain.loc[rain.index.month == i.month]['inches'][0]\n",
    "\n",
    "        # adding type\n",
    "        entry['type'] = kind\n",
    "\n",
    "        # adding stage\n",
    "        if(i in dfs.index):\n",
    "            entry['stage'] = dfs.loc[i][0]\n",
    "            start = dfs.loc[i][0]           \n",
    "        else:\n",
    "            # check for the Harevest day \n",
    "            # if Harvest day has been recorded\n",
    "            # no need to record the other dates\n",
    "            if(start == np.int64(3)):\n",
    "                start = np.nan\n",
    "                entry['stage'] = start\n",
    "            else:\n",
    "                entry['stage'] = start\n",
    "\n",
    "        dataset = dataset.append(entry, sort=False)\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(stages[stages['types']==1].shape[0]):\n",
    "#     print((i%4)+1)\n",
    "\n",
    "\n",
    "# use = make_dataset(1,'2010-01-01', '2017-12-31')\n",
    "\n",
    "# use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pickleing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, name):\n",
    "    path = \"../data/\"\n",
    "    with open(path+\"pkl/\"+name+\".pkl\", 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "        \n",
    "    data.to_csv(path+'csv/'+name+'.csv',sep=\",\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_data(use,'2010to2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/pkl/dataset.pkl', 'rb') as data:\n",
    "#     dataset = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/csv/2010to2017.csv')\n",
    "df = df.set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for unknow values\n",
    "# use.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../script/cleanup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = cleanup(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### try to implement categorical binning \n",
    "#### Split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the label can be the number of week\n",
    "# and it can be a regression problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Also look at the overall statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = train_dataset.describe()\n",
    "train_stats.pop(\"Harvest\")\n",
    "train_stats = train_stats.transpose()\n",
    "train_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split features from labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_dataset.pop('Harvest')\n",
    "test_labels = test_dataset.pop('Harvest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def norm(x):\n",
    "#     return (x - train_stats['mean'])/train_stats['std']\n",
    "# normed_train_data = norm(train_dataset)\n",
    "# normed_test_data = norm(test_dataset)\n",
    "from sklearn import preprocessing\n",
    "def standardize_data(df):\n",
    "    \n",
    "    X_scaled = preprocessing.scale(df[['minTemp', 'maxTemp', 'Eto_values','DayPrecip','WindSpd','DaySolRad',\n",
    "                                       'rain','Month','Day', 'Bud', 'Bloom','Veraison']])\n",
    " \n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=['minTemp', 'maxTemp', 'Eto_values','DayPrecip','WindSpd','DaySolRad',\n",
    "                                                  'rain','Month','Day','Bud','Bloom','Veraison'])\n",
    "#     df_scale = pd.concat([X_scaled_df,\n",
    "#                          df['Bud'], df['Bloom'], df['Veraison']],axis=1) # join='inner')\n",
    "\n",
    "    return X_scaled_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_scale = standardize_data(train_dataset)\n",
    "df_test_scale = standardize_data(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_scale.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation=tf.nn.relu, input_shape=[len(df_train_scale.keys())]),\n",
    "        layers.Dense(64, activation=tf.nn.relu),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "    model.compile(loss='mse',\n",
    "             optimizer=optimizer,\n",
    "             metrics=['mae','mse'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "example_batch = df_train_scale[:10]\n",
    "example_result = model.predict(example_batch)\n",
    "example_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# train_dataset.head()\n",
    "df_train_scale[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class PrintDot(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch % 100 == 0: print('')\n",
    "        print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "history = model.fit(\n",
    "    df_train_scale, train_labels,\n",
    "    epochs=EPOCHS, validation_split = 0.2, verbose=0, callbacks=[PrintDot()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error [Harverst]')\n",
    "    plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
    "            label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
    "            label='Val Error')\n",
    "    plt.legend()\n",
    "    plt.ylim([0,0.25])\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Square Error [Harverst^2]')\n",
    "    plt.plot(hist['epoch'], hist['mean_squared_error'],\n",
    "            label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_mean_squared_error'],\n",
    "            label='Val Error')\n",
    "    plt.legend()\n",
    "    plt.ylim([0,0.0625])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(df_test_scale, test_labels)\n",
    "print(\"Accuracy: %.2f%%\"%(scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "history = model.fit(df_train_scale,train_labels, epochs=EPOCHS, validation_split= 0.2, verbose=0, callbacks=[early_stop, PrintDot()])\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "loss, mae, mse = model.evaluate(df_test_scale, test_labels, verbose=0)\n",
    "print(\"Testing set Mean Abs Error: {:5.2f} Haverst\".format(mae))\n",
    "print(\"Testing set Mean square Error: {:5.2f} Haverst\".format(mse))\n",
    "print(\"Testing set loss: {:5.2f} Haverst\".format(loss))\n",
    "scores = model.evaluate(df_test_scale, test_labels)\n",
    "print(\"Accuracy: %.2f%%\"%(scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_predictions = model.predict(df_test_scale).flatten()\n",
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_labels.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(test_labels, test_predictions)\n",
    "plt.xlabel('True Values [Harvest]')\n",
    "plt.ylabel('Predictions [Harvest]')\n",
    "plt.axis('equal')\n",
    "plt.axis('square')\n",
    "plt.xlim([0,plt.xlim()[1]])\n",
    "plt.ylim([0,plt.ylim()[1]])\n",
    "_ = plt.plot([-10,10], [-10,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "error = test_predictions - test_labels\n",
    "plt.hist(error, bins = 25)\n",
    "plt.xlabel(\"Prediction Error [Harvest]\")\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
